# Algorithmic_bias
## Project Overview

**Unequal Recognition: Facial Recognition Bias in Policing** is a data-driven investigation into whether facial recognition algorithms used in U.S. law enforcement exhibit higher error rates for certain racial and gender groupsâ€”and whether these inaccuracies align with increased surveillance or enforcement actions against those groups.

This project builds on existing research that has revealed disproportionately high false positive rates for Black individuals and women in commercial facial recognition systems. By combining **algorithm audit reports** with **real-world policing data**, this project aims to explore the relationship between biased AI systems and potential discriminatory outcomes in policing practices.

---

## Research Question

**Do facial recognition algorithms used in U.S. law enforcement systems show higher error rates for certain racial or gender groups, and do these inaccuracies align with increased surveillance or enforcement actions against those groups?**

---

## Data Sources

- **NIST FRVT** â€“ Error rates by race and gender for face recognition vendors  
- **MIT Media Lab Gender Shades** â€“ Intersectional analysis of commercial face recognition accuracy  
- **Stanford Open Policing Project** â€“ Traffic stop data by race/gender across U.S. states  
- **NYPD Stop-and-Frisk Dataset** â€“ Historical police encounter data disaggregated by race/gender  
- **ACLU & Georgetown Reports** â€“ Documentation of cities and states using facial recognition  

---

## Goals

- Quantify demographic disparities in facial recognition error rates  
- Identify cities/states that use facial recognition in policing  
- Compare policing patterns (e.g., stops, arrests) in relation to algorithmic bias  
- Explore whether AI-driven surveillance disproportionately impacts marginalized communities

---

## ðŸ“Œ License

This project uses only public and open datasets. Please credit original data providers such as NIST, MIT Media Lab, and the Stanford Open Policing Project.
